{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from inception_blocks import *\n",
    "K.set_image_data_format('channels_first')\n",
    "from fr_utils import *\n",
    "from keras.layers import merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss as defined by formula (3)\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor images, of shape (None, 128)\n",
    "            positive -- the encodings for the positive images, of shape (None, 128)\n",
    "            negative -- the encodings for the negative images, of shape (None, 128)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    print(\"Inside triplet loss\")\n",
    "    \n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    print(anchor)\n",
    "    print(positive)\n",
    "    print(negative)\n",
    "    ### START CODE HERE ### (Ëœ 4 lines)\n",
    "    # Step 1: Compute the (encoding) distance between the anchor and the positive, you will need to sum over axis=-1\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)),axis=-1)\n",
    "    # Step 2: Compute the (encoding) distance between the anchor and the negative, you will need to sum over axis=-1\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)),axis=-1)\n",
    "    # Step 3: subtract the two previous distances and add alpha.\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist,neg_dist),alpha)\n",
    "    # Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.\n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss, 0.0))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    print(\"Entering the function\")\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faceRecoModel(input_shape):\n",
    "    \"\"\"\n",
    "    Implementation of the Inception model used for FaceNet\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "        \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    # First Block\n",
    "    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1')(X)\n",
    "    X = BatchNormalization(axis = 1, name = 'bn1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Zero-Padding + MAXPOOL\n",
    "    X = ZeroPadding2D((1, 1))(X)\n",
    "    X = MaxPooling2D((3, 3), strides = 2)(X)\n",
    "    \n",
    "    # Second Block\n",
    "    X = Conv2D(64, (1, 1), strides = (1, 1), name = 'conv2')(X)\n",
    "    X = BatchNormalization(axis = 1, epsilon=0.00001, name = 'bn2')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Zero-Padding + MAXPOOL\n",
    "    X = ZeroPadding2D((1, 1))(X)\n",
    "\n",
    "    # Second Block\n",
    "    X = Conv2D(192, (3, 3), strides = (1, 1), name = 'conv3')(X)\n",
    "    X = BatchNormalization(axis = 1, epsilon=0.00001, name = 'bn3')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Zero-Padding + MAXPOOL\n",
    "    X = ZeroPadding2D((1, 1))(X)\n",
    "    \n",
    "    X = MaxPooling2D(pool_size = 3, strides = 2)(X)\n",
    "    \n",
    "    X = AveragePooling2D(pool_size=(3, 3), strides=(1, 1), data_format='channels_first')(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(128, name='dense_layer')(X)\n",
    "    \n",
    "    # L2 normalization\n",
    "    X = Lambda(lambda  x: K.l2_normalize(x,axis=1))(X)\n",
    "    \n",
    "   \n",
    "          \n",
    "    # Create model instance\n",
    "    model = Model(inputs = X_input, outputs = X, name='FaceRecoModel')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_FLOATX = 'float32'\n",
    "def variable(value, dtype=_FLOATX, name=None):\n",
    "    v = tf.Variable(np.asarray(value, dtype=dtype), name=name)\n",
    "    _get_session().run(v.initializer)\n",
    "    return v\n",
    "\n",
    "def shape(x):\n",
    "    return x.get_shape()\n",
    "\n",
    "def square(x):\n",
    "    return tf.square(x)\n",
    "\n",
    "def zeros(shape, dtype=_FLOATX, name=None):\n",
    "    return variable(np.zeros(shape), dtype, name)\n",
    "\n",
    "def concatenate(tensors, axis=-1):\n",
    "    if axis < 0:\n",
    "        axis = axis % len(tensors[0].get_shape())\n",
    "    return tf.concat(axis, tensors)\n",
    "\n",
    "def LRN2D(x):\n",
    "    return tf.nn.lrn(x, alpha=1e-4, beta=0.75)\n",
    "\n",
    "def conv2d_bn(x,\n",
    "              layer=None,\n",
    "              cv1_out=None,\n",
    "              cv1_filter=(1, 1),\n",
    "              cv1_strides=(1, 1),\n",
    "              cv2_out=None,\n",
    "              cv2_filter=(3, 3),\n",
    "              cv2_strides=(1, 1),\n",
    "              padding=None):\n",
    "    num = '' if cv2_out == None else '1'\n",
    "    tensor = Conv2D(cv1_out, cv1_filter, strides=cv1_strides, data_format='channels_first', name=layer+'_conv'+num)(x)\n",
    "    tensor = BatchNormalization(axis=1, epsilon=0.00001, name=layer+'_bn'+num)(tensor)\n",
    "    tensor = Activation('relu')(tensor)\n",
    "    if padding == None:\n",
    "        return tensor\n",
    "    tensor = ZeroPadding2D(padding=padding, data_format='channels_first')(tensor)\n",
    "    if cv2_out == None:\n",
    "        return tensor\n",
    "    tensor = Conv2D(cv2_out, cv2_filter, strides=cv2_strides, data_format='channels_first', name=layer+'_conv'+'2')(tensor)\n",
    "    tensor = BatchNormalization(axis=1, epsilon=0.00001, name=layer+'_bn'+'2')(tensor)\n",
    "    tensor = Activation('relu')(tensor)\n",
    "    return tensor\n",
    "\n",
    "WEIGHTS_mod = [\n",
    "  'conv1', 'bn1', 'conv2', 'bn2', 'conv3', 'bn3'\n",
    "]\n",
    "conv_shape = {\n",
    "  'conv1': [64, 3, 7, 7],\n",
    "  'conv2': [64, 64, 1, 1],\n",
    "  'conv3': [192, 64, 3, 3]\n",
    "}\n",
    "\n",
    "def load_weights_from_FaceNet(FRmodel):\n",
    "    # Load weights from csv files (which was exported from Openface torch model)\n",
    "    weights = WEIGHTS_mod\n",
    "    weights_dict = load_weights()\n",
    "\n",
    "    # Set layer weights of the model\n",
    "    for name in weights:\n",
    "        if FRmodel.get_layer(name) != None:\n",
    "            FRmodel.get_layer(name).set_weights(weights_dict[name])\n",
    "        elif model.get_layer(name) != None:\n",
    "            model.get_layer(name).set_weights(weights_dict[name])\n",
    "\n",
    "def load_weights():\n",
    "    # Set weights path\n",
    "    dirPath = './weights'\n",
    "    fileNames = filter(lambda f: not f.startswith('.'), os.listdir(dirPath))\n",
    "    paths = {}\n",
    "    weights_dict = {}\n",
    "\n",
    "    for n in fileNames:\n",
    "        paths[n.replace('.csv', '')] = dirPath + '/' + n\n",
    "\n",
    "    for name in WEIGHTS_mod:\n",
    "        if 'conv' in name:\n",
    "            conv_w = genfromtxt(paths[name + '_w'], delimiter=',', dtype=None)\n",
    "            conv_w = np.reshape(conv_w, conv_shape[name])\n",
    "            conv_w = np.transpose(conv_w, (2, 3, 1, 0))\n",
    "            conv_b = genfromtxt(paths[name + '_b'], delimiter=',', dtype=None)\n",
    "            weights_dict[name] = [conv_w, conv_b]     \n",
    "        elif 'bn' in name:\n",
    "            bn_w = genfromtxt(paths[name + '_w'], delimiter=',', dtype=None)\n",
    "            bn_b = genfromtxt(paths[name + '_b'], delimiter=',', dtype=None)\n",
    "            bn_m = genfromtxt(paths[name + '_m'], delimiter=',', dtype=None)\n",
    "            bn_v = genfromtxt(paths[name + '_v'], delimiter=',', dtype=None)\n",
    "            weights_dict[name] = [bn_w, bn_b, bn_m, bn_v]\n",
    "        elif 'dense' in name:\n",
    "            dense_w = genfromtxt(dirPath+'/dense_w.csv', delimiter=',', dtype=None)\n",
    "            dense_w = np.reshape(dense_w, (128, 736))\n",
    "            dense_w = np.transpose(dense_w, (1, 0))\n",
    "            dense_b = genfromtxt(dirPath+'/dense_b.csv', delimiter=',', dtype=None)\n",
    "            weights_dict[name] = [dense_w, dense_b]\n",
    "\n",
    "    return weights_dict\n",
    "\n",
    "def img_to_encoding(image_path, model):\n",
    "    img1 = cv2.imread(image_path, 1)\n",
    "    img = img1[...,::-1]\n",
    "    img = np.around(np.transpose(img, (2,0,1))/255.0, decimals=12)\n",
    "    x_train = np.array([img])\n",
    "    embedding = model.predict_on_batch(x_train)\n",
    "    flat_input=embedding.flatten()\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside triplet loss\n",
      "Tensor(\"loss/lambda_1_loss/strided_slice:0\", shape=(128,), dtype=float32)\n",
      "Tensor(\"loss/lambda_1_loss/strided_slice_1:0\", shape=(128,), dtype=float32)\n",
      "Tensor(\"loss/lambda_1_loss/strided_slice_2:0\", shape=(128,), dtype=float32)\n",
      "Entering the function\n"
     ]
    }
   ],
   "source": [
    "\n",
    "FRmodel = faceRecoModel(input_shape=(3, 350, 350))\n",
    "FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])\n",
    "load_weights_from_FaceNet(FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array([['Image1.png','Image1_modified.png','Image2.png'],\n",
    "   ['Image1.png','Image1_modified1.png','Image3.png'],\n",
    "   ['Image1.png','Image1_modified2.png','Image4.png'],\n",
    "   ['Image1.png','Image1_modified3.png','Image5.png']]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_array(image_path):\n",
    "    img1 = cv2.imread(image_path, 1)\n",
    "    img = img1[...,::-1]\n",
    "    img = np.around(np.transpose(img, (2,0,1))/255.0, decimals=12)\n",
    "    iarray = np.array([img]) \n",
    "    return iarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   #[Not used]\n",
    "    \n",
    "    positive_item_input = Input((1, ), name='positive_item_input')\n",
    "    negative_item_input = Input((1, ), name='negative_item_input')\n",
    "    user_input = Input((1, ), name='user_input')\n",
    "    # Shared embedding layer for positive and negative items\n",
    "    # item_embedding_layer = Embedding(\n",
    "    # num_items, latent_dim, name='item_embedding', input_length=1)\n",
    "    user_input = Input((1, ), name='user_input')\n",
    "    # positive_item_embedding = Flatten()(positive_item_input)\n",
    "    #negative_item_embedding = Flatten()(negative_item_input)\n",
    "    user_embedding = Flatten()(user_input)\n",
    "    print (user_embedding)\n",
    "    loss = np.zeros((user_input.shape))\n",
    "    \n",
    "    model = Model (input=[ user_input,positive_item_input,negative_item_input],\n",
    "                   output=loss)\n",
    "    model.compile(loss=triplet_loss, optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   #[Not used]\n",
    "X=np.array([['Image1.png','Image1_modified.png','Image2.png'],\n",
    "   ['Image1.png','Image1_modified1.png','Image3.png'],\n",
    "   ['Image1.png','Image1_modified2.png','Image4.png'],\n",
    "   ['Image1.png','Image1_modified3.png','Image5.png']]\n",
    "    )\n",
    "X_inputs=[]\n",
    "for uid,pid,nid in X:\n",
    "     X_triplet = {\n",
    "        'user_input':image_to_array(\"images/\"+uid),\n",
    "        'positive_item_input':image_to_array(\"images/\"+pid),\n",
    "        'negative_item_input':image_to_array(\"images/\"+nid)\n",
    "     }\n",
    "     X_inputs=X_inputs.append(X_triplet)\n",
    "    \n",
    "FRmodel.fit(inputs=X_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_images(image_path_original,image_path_check):\n",
    "    \n",
    "    imageA=mpimg.imread(image_path_original)\n",
    "    imageB=mpimg.imread(image_path_check)\n",
    "    fig = plt.figure(\"Comparison\")\n",
    " \n",
    "    # show first image\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(imageA, cmap = plt.cm.gray)\n",
    "    plt.axis(\"off\")\n",
    " \n",
    "    # show the second image\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    plt.imshow(imageB, cmap = plt.cm.gray)\n",
    "    plt.axis(\"off\")\n",
    " \n",
    "    # show the images\n",
    "    plt.show()\n",
    "\n",
    "def verify(image_path_original, image_path_check, model): \n",
    "    \n",
    "    encoding_original = img_to_encoding(image_path_original,model)\n",
    "    encoding_check = img_to_encoding(image_path_check,model)\n",
    "    \n",
    "    dist = np.linalg.norm(encoding_original-encoding_check)\n",
    "    dist = dist/256\n",
    "    # setup the figure\n",
    "    \n",
    "    show_images(image_path_original,image_path_check)\n",
    "    \n",
    "    print (\"Distance is \" , dist)\n",
    "    if dist < 1:\n",
    "        print(\"It's similar\")\n",
    "                \n",
    "    else:\n",
    "        print(\"It's not similar\")\n",
    "    return dist\n",
    "\n",
    "def verify_ml_approach(image_path_original,image_path_check):\n",
    "\n",
    "    dist = verify(image_path_original, image_path_check, FRmodel)\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_ml_approach(\"images/image1.png\", \"images/image3.png\")\n",
    "#verify_ml_approach(\"images/Original.png\", \"images/Negative.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "verify_ml_approach(\"images/image1.png\", \"images/image1_modified.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_ml_approach(\"images/image1.png\", \"images/image1_modified1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_ml_approach(\"images/image1.png\", \"images/image1_modified2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_ml_approach(\"images/image1.png\", \"images/image1_modified3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_ml_approach(\"images/image1.png\",\"images/image9.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_ml_approach(\"images/image1.png\",\"images/image6.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "database = {}\n",
    "database[\"Image1\"] = img_to_encoding(\"images/Image1.png\", FRmodel)\n",
    "database[\"Image2\"] = img_to_encoding(\"images/Image2.png\", FRmodel)\n",
    "database[\"Image3\"] = img_to_encoding(\"images/Image3.png\", FRmodel)\n",
    "database[\"Image4\"] = img_to_encoding(\"images/Image4.png\", FRmodel)\n",
    "database[\"Image5\"] = img_to_encoding(\"images/Image5.png\", FRmodel)\n",
    "database[\"Image6\"] = img_to_encoding(\"images/Image6.png\", FRmodel)\n",
    "database[\"Image7\"] = img_to_encoding(\"images/Image7.png\", FRmodel)\n",
    "database[\"Image8\"] = img_to_encoding(\"images/Image8.png\", FRmodel)\n",
    "database[\"Image9\"] = img_to_encoding(\"images/Image9.png\", FRmodel)\n",
    "database[\"Image10\"] = img_to_encoding(\"images/Image10.png\", FRmodel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Image1' 'Image7']\n",
      " ['Image1' 'Image8']\n",
      " ['Image1' 'Image1_modified']\n",
      " ['Image1' 'Image4']\n",
      " ['Image1' 'Image1_modified1']\n",
      " ['Image1' 'Image5']\n",
      " ['Image1' 'Image1_modified2']\n",
      " ['Image1' 'Image10']\n",
      " ['Image1' 'Image1_modified3']\n",
      " ['Image1' 'Image2']\n",
      " ['Image1' 'Image6']\n",
      " ['Image1' 'Image10']\n",
      " ['Image1' 'Image3']\n",
      " ['Image1' 'Image9']]\n",
      "[0, 0, 6, 0, 8, 0, 9, 0, 10, 2, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "dataset = np.genfromtxt(\"data.csv\", dtype=\"U\" ,delimiter=\",\" )\n",
    "\n",
    "X = dataset[:,0:2]\n",
    "Y = dataset[:,2]\n",
    "\n",
    "Y= [int(i) for i in Y]\n",
    "\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getimagePath(name):\n",
    "    return('images/'+str(name)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimpleModel():\n",
    "    inputs=Input(shape=(128,))\n",
    "    # a layer instance is callable on a tensor, and returns a tensor\n",
    "    x = Dense(64, activation='relu')(inputs)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    predictions = Dense(11, activation='softmax')(x)\n",
    "\n",
    "    # This creates a model that includes\n",
    "    # the Input layer and three Dense layers\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSeqModel():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(11, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "[2 3 4 0]\n",
      "(4, 6)\n",
      "[[0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "nb_classes = 6\n",
    "targets = np.array([[2, 3, 4, 0]]).reshape(-1)\n",
    "print(targets.shape)\n",
    "print(targets)\n",
    "one_hot_targets = np.eye(nb_classes)[targets]\n",
    "print(one_hot_targets.shape)\n",
    "print(one_hot_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=14\n",
    "classes=11\n",
    "dat=np.zeros((1,128))\n",
    "data=np.zeros((m,128))\n",
    "labels=np.zeros((m,classes))\n",
    "row=0\n",
    "for [ori,chk] in X:\n",
    "    ori_encoding=img_to_encoding(getimagePath(ori), FRmodel)\n",
    "    chk_encoding=img_to_encoding(getimagePath(chk), FRmodel)\n",
    "    dat=(ori_encoding-chk_encoding)\n",
    "    data[row,:]=dat\n",
    "    labels[row,Y[row]]=1\n",
    "    row=row+1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 2.3877 - acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "14/14 [==============================] - 0s 142us/step - loss: 2.3467 - acc: 0.5714\n",
      "Epoch 3/100\n",
      "14/14 [==============================] - 0s 142us/step - loss: 2.3104 - acc: 0.5714\n",
      "Epoch 4/100\n",
      "14/14 [==============================] - 0s 140us/step - loss: 2.2759 - acc: 0.5714\n",
      "Epoch 5/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 2.2444 - acc: 0.5714\n",
      "Epoch 6/100\n",
      "14/14 [==============================] - 0s 142us/step - loss: 2.2123 - acc: 0.5714\n",
      "Epoch 7/100\n",
      "14/14 [==============================] - 0s 143us/step - loss: 2.1801 - acc: 0.5000\n",
      "Epoch 8/100\n",
      "14/14 [==============================] - 0s 143us/step - loss: 2.1489 - acc: 0.5000\n",
      "Epoch 9/100\n",
      "14/14 [==============================] - 0s 143us/step - loss: 2.1152 - acc: 0.5000\n",
      "Epoch 10/100\n",
      "14/14 [==============================] - 0s 142us/step - loss: 2.0820 - acc: 0.5000\n",
      "Epoch 11/100\n",
      "14/14 [==============================] - 0s 143us/step - loss: 2.0465 - acc: 0.5000\n",
      "Epoch 12/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 2.0110 - acc: 0.5000\n",
      "Epoch 13/100\n",
      "14/14 [==============================] - 0s 142us/step - loss: 1.9749 - acc: 0.5000\n",
      "Epoch 14/100\n",
      "14/14 [==============================] - 0s 285us/step - loss: 1.9386 - acc: 0.5000\n",
      "Epoch 15/100\n",
      "14/14 [==============================] - 0s 285us/step - loss: 1.9019 - acc: 0.5000\n",
      "Epoch 16/100\n",
      "14/14 [==============================] - 0s 428us/step - loss: 1.8646 - acc: 0.5000\n",
      "Epoch 17/100\n",
      "14/14 [==============================] - 0s 285us/step - loss: 1.8271 - acc: 0.5000\n",
      "Epoch 18/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 1.7905 - acc: 0.5000\n",
      "Epoch 19/100\n",
      "14/14 [==============================] - 0s 143us/step - loss: 1.7544 - acc: 0.5000\n",
      "Epoch 20/100\n",
      "14/14 [==============================] - 0s 499us/step - loss: 1.7188 - acc: 0.5000\n",
      "Epoch 21/100\n",
      "14/14 [==============================] - 0s 285us/step - loss: 1.6844 - acc: 0.5000\n",
      "Epoch 22/100\n",
      "14/14 [==============================] - 0s 427us/step - loss: 1.6506 - acc: 0.5000\n",
      "Epoch 23/100\n",
      "14/14 [==============================] - 0s 499us/step - loss: 1.6184 - acc: 0.5000\n",
      "Epoch 24/100\n",
      "14/14 [==============================] - 0s 285us/step - loss: 1.5870 - acc: 0.5000\n",
      "Epoch 25/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 1.5570 - acc: 0.5000\n",
      "Epoch 26/100\n",
      "14/14 [==============================] - 0s 356us/step - loss: 1.5291 - acc: 0.5000\n",
      "Epoch 27/100\n",
      "14/14 [==============================] - 0s 427us/step - loss: 1.5019 - acc: 0.5000\n",
      "Epoch 28/100\n",
      "14/14 [==============================] - 0s 142us/step - loss: 1.4764 - acc: 0.5000\n",
      "Epoch 29/100\n",
      "14/14 [==============================] - 0s 499us/step - loss: 1.4514 - acc: 0.5000\n",
      "Epoch 30/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 1.4282 - acc: 0.5000\n",
      "Epoch 31/100\n",
      "14/14 [==============================] - 0s 641us/step - loss: 1.4057 - acc: 0.5000\n",
      "Epoch 32/100\n",
      "14/14 [==============================] - 0s 782us/step - loss: 1.3841 - acc: 0.5000\n",
      "Epoch 33/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 1.3634 - acc: 0.5000\n",
      "Epoch 34/100\n",
      "14/14 [==============================] - 0s 142us/step - loss: 1.3437 - acc: 0.5000\n",
      "Epoch 35/100\n",
      "14/14 [==============================] - 0s 784us/step - loss: 1.3243 - acc: 0.5000\n",
      "Epoch 36/100\n",
      "14/14 [==============================] - 0s 357us/step - loss: 1.3057 - acc: 0.5000\n",
      "Epoch 37/100\n",
      "14/14 [==============================] - 0s 499us/step - loss: 1.2875 - acc: 0.5000\n",
      "Epoch 38/100\n",
      "14/14 [==============================] - 0s 427us/step - loss: 1.2700 - acc: 0.5000\n",
      "Epoch 39/100\n",
      "14/14 [==============================] - 0s 499us/step - loss: 1.2525 - acc: 0.5714\n",
      "Epoch 40/100\n",
      "14/14 [==============================] - 0s 285us/step - loss: 1.2351 - acc: 0.5714\n",
      "Epoch 41/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 1.2181 - acc: 0.5714\n",
      "Epoch 42/100\n",
      "14/14 [==============================] - 0s 215us/step - loss: 1.2012 - acc: 0.5714\n",
      "Epoch 43/100\n",
      "14/14 [==============================] - 0s 143us/step - loss: 1.1847 - acc: 0.5714\n",
      "Epoch 44/100\n",
      "14/14 [==============================] - 0s 143us/step - loss: 1.1679 - acc: 0.5714\n",
      "Epoch 45/100\n",
      "14/14 [==============================] - 0s 285us/step - loss: 1.1514 - acc: 0.5714\n",
      "Epoch 46/100\n",
      "14/14 [==============================] - 0s 356us/step - loss: 1.1351 - acc: 0.5714\n",
      "Epoch 47/100\n",
      "14/14 [==============================] - 0s 356us/step - loss: 1.1193 - acc: 0.5714\n",
      "Epoch 48/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 1.1035 - acc: 0.5714\n",
      "Epoch 49/100\n",
      "14/14 [==============================] - 0s 285us/step - loss: 1.0874 - acc: 0.6429\n",
      "Epoch 50/100\n",
      "14/14 [==============================] - 0s 142us/step - loss: 1.0717 - acc: 0.6429\n",
      "Epoch 51/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 1.0557 - acc: 0.6429\n",
      "Epoch 52/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 1.0402 - acc: 0.6429\n",
      "Epoch 53/100\n",
      "14/14 [==============================] - 0s 358us/step - loss: 1.0249 - acc: 0.6429\n",
      "Epoch 54/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 1.0094 - acc: 0.6429\n",
      "Epoch 55/100\n",
      "14/14 [==============================] - 0s 213us/step - loss: 0.9941 - acc: 0.6429\n",
      "Epoch 56/100\n",
      "14/14 [==============================] - 0s 143us/step - loss: 0.9786 - acc: 0.6429\n",
      "Epoch 57/100\n",
      "14/14 [==============================] - 0s 143us/step - loss: 0.9631 - acc: 0.6429\n",
      "Epoch 58/100\n",
      "14/14 [==============================] - 0s 71us/step - loss: 0.9477 - acc: 0.6429\n",
      "Epoch 59/100\n",
      "14/14 [==============================] - 0s 142us/step - loss: 0.9321 - acc: 0.6429\n",
      "Epoch 60/100\n",
      "14/14 [==============================] - 0s 142us/step - loss: 0.9164 - acc: 0.7143\n",
      "Epoch 61/100\n",
      "14/14 [==============================] - 0s 499us/step - loss: 0.9008 - acc: 0.7143\n",
      "Epoch 62/100\n",
      "14/14 [==============================] - 0s 641us/step - loss: 0.8859 - acc: 0.7143\n",
      "Epoch 63/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 0.8709 - acc: 0.7143\n",
      "Epoch 64/100\n",
      "14/14 [==============================] - 0s 213us/step - loss: 0.8556 - acc: 0.7143\n",
      "Epoch 65/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 0.8408 - acc: 0.7143\n",
      "Epoch 66/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 0.8261 - acc: 0.7143\n",
      "Epoch 67/100\n",
      "14/14 [==============================] - 0s 428us/step - loss: 0.8111 - acc: 0.7143\n",
      "Epoch 68/100\n",
      "14/14 [==============================] - 0s 142us/step - loss: 0.7964 - acc: 0.7143\n",
      "Epoch 69/100\n",
      "14/14 [==============================] - 0s 142us/step - loss: 0.7819 - acc: 0.7143\n",
      "Epoch 70/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 0.7673 - acc: 0.7143\n",
      "Epoch 71/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 0.7531 - acc: 0.7143\n",
      "Epoch 72/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 0.7390 - acc: 0.7143\n",
      "Epoch 73/100\n",
      "14/14 [==============================] - 0s 142us/step - loss: 0.7245 - acc: 0.7143\n",
      "Epoch 74/100\n",
      "14/14 [==============================] - 0s 143us/step - loss: 0.7108 - acc: 0.7143\n",
      "Epoch 75/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 0.6967 - acc: 0.7143\n",
      "Epoch 76/100\n",
      "14/14 [==============================] - 0s 142us/step - loss: 0.6832 - acc: 0.7857\n",
      "Epoch 77/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 0.6691 - acc: 0.7857\n",
      "Epoch 78/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 0.6557 - acc: 0.7857\n",
      "Epoch 79/100\n",
      "14/14 [==============================] - 0s 356us/step - loss: 0.6423 - acc: 0.7857\n",
      "Epoch 80/100\n",
      "14/14 [==============================] - 0s 712us/step - loss: 0.6289 - acc: 0.8571\n",
      "Epoch 81/100\n",
      "14/14 [==============================] - 0s 429us/step - loss: 0.6159 - acc: 0.7857\n",
      "Epoch 82/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 0.6029 - acc: 0.8571\n",
      "Epoch 83/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 0.5901 - acc: 0.9286\n",
      "Epoch 84/100\n",
      "14/14 [==============================] - 0s 285us/step - loss: 0.5779 - acc: 0.9286\n",
      "Epoch 85/100\n",
      "14/14 [==============================] - 0s 285us/step - loss: 0.5657 - acc: 0.9286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100\n",
      "14/14 [==============================] - 0s 356us/step - loss: 0.5534 - acc: 0.9286\n",
      "Epoch 87/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 0.5414 - acc: 0.9286\n",
      "Epoch 88/100\n",
      "14/14 [==============================] - 0s 285us/step - loss: 0.5297 - acc: 0.9286\n",
      "Epoch 89/100\n",
      "14/14 [==============================] - 0s 356us/step - loss: 0.5182 - acc: 0.9286\n",
      "Epoch 90/100\n",
      "14/14 [==============================] - 0s 143us/step - loss: 0.5070 - acc: 0.9286\n",
      "Epoch 91/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 0.4958 - acc: 0.9286\n",
      "Epoch 92/100\n",
      "14/14 [==============================] - 0s 285us/step - loss: 0.4851 - acc: 0.9286\n",
      "Epoch 93/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 0.4746 - acc: 0.9286\n",
      "Epoch 94/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 0.4638 - acc: 0.9286\n",
      "Epoch 95/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 0.4540 - acc: 0.9286\n",
      "Epoch 96/100\n",
      "14/14 [==============================] - 0s 142us/step - loss: 0.4438 - acc: 0.9286\n",
      "Epoch 97/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 0.4343 - acc: 0.9286\n",
      "Epoch 98/100\n",
      "14/14 [==============================] - 0s 214us/step - loss: 0.4247 - acc: 0.9286\n",
      "Epoch 99/100\n",
      "14/14 [==============================] - 0s 142us/step - loss: 0.4151 - acc: 0.9286\n",
      "Epoch 100/100\n",
      "14/14 [==============================] - 0s 143us/step - loss: 0.4059 - acc: 0.9286\n",
      "14/14 [==============================] - 0s 11ms/step\n",
      "Score is  [0.3970050811767578, 0.9285714030265808]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "Smodel=getSimpleModel()\n",
    "    \n",
    "Smodel.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "Smodel.fit(data,labels,epochs=100 )  # starts training\n",
    "score = Smodel.evaluate(data,labels)\n",
    "print(\"Score is \",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
