{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from inception_blocks import *\n",
    "K.set_image_data_format('channels_first')\n",
    "from fr_utils import *\n",
    "from keras.layers import merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss as defined by formula (3)\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor images, of shape (None, 128)\n",
    "            positive -- the encodings for the positive images, of shape (None, 128)\n",
    "            negative -- the encodings for the negative images, of shape (None, 128)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    print(\"Inside triplet loss\")\n",
    "    \n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    print(anchor)\n",
    "    print(positive)\n",
    "    print(negative)\n",
    "    ### START CODE HERE ### (Ëœ 4 lines)\n",
    "    # Step 1: Compute the (encoding) distance between the anchor and the positive, you will need to sum over axis=-1\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)),axis=-1)\n",
    "    # Step 2: Compute the (encoding) distance between the anchor and the negative, you will need to sum over axis=-1\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)),axis=-1)\n",
    "    # Step 3: subtract the two previous distances and add alpha.\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist,neg_dist),alpha)\n",
    "    # Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.\n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss, 0.0))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    print(\"Entering the function\")\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faceRecoModel(input_shape):\n",
    "    \"\"\"\n",
    "    Implementation of the Inception model used for FaceNet\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "        \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    # First Block\n",
    "    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1')(X)\n",
    "    X = BatchNormalization(axis = 1, name = 'bn1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Zero-Padding + MAXPOOL\n",
    "    X = ZeroPadding2D((1, 1))(X)\n",
    "    X = MaxPooling2D((3, 3), strides = 2)(X)\n",
    "    \n",
    "    # Second Block\n",
    "    X = Conv2D(64, (1, 1), strides = (1, 1), name = 'conv2')(X)\n",
    "    X = BatchNormalization(axis = 1, epsilon=0.00001, name = 'bn2')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Zero-Padding + MAXPOOL\n",
    "    X = ZeroPadding2D((1, 1))(X)\n",
    "\n",
    "    # Second Block\n",
    "    X = Conv2D(192, (3, 3), strides = (1, 1), name = 'conv3')(X)\n",
    "    X = BatchNormalization(axis = 1, epsilon=0.00001, name = 'bn3')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Zero-Padding + MAXPOOL\n",
    "    X = ZeroPadding2D((1, 1))(X)\n",
    "    \n",
    "    X = MaxPooling2D(pool_size = 3, strides = 2)(X)\n",
    "    \n",
    "    X = AveragePooling2D(pool_size=(3, 3), strides=(1, 1), data_format='channels_first')(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(128, name='dense_layer')(X)\n",
    "    \n",
    "    # L2 normalization\n",
    "    X = Lambda(lambda  x: K.l2_normalize(x,axis=1))(X)\n",
    "    \n",
    "    print (X.shape)\n",
    "          \n",
    "    # Create model instance\n",
    "    model = Model(inputs = X_input, outputs = X, name='FaceRecoModel')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "_FLOATX = 'float32'\n",
    "def variable(value, dtype=_FLOATX, name=None):\n",
    "    v = tf.Variable(np.asarray(value, dtype=dtype), name=name)\n",
    "    _get_session().run(v.initializer)\n",
    "    return v\n",
    "\n",
    "def shape(x):\n",
    "    return x.get_shape()\n",
    "\n",
    "def square(x):\n",
    "    return tf.square(x)\n",
    "\n",
    "def zeros(shape, dtype=_FLOATX, name=None):\n",
    "    return variable(np.zeros(shape), dtype, name)\n",
    "\n",
    "def concatenate(tensors, axis=-1):\n",
    "    if axis < 0:\n",
    "        axis = axis % len(tensors[0].get_shape())\n",
    "    return tf.concat(axis, tensors)\n",
    "\n",
    "def LRN2D(x):\n",
    "    return tf.nn.lrn(x, alpha=1e-4, beta=0.75)\n",
    "\n",
    "def conv2d_bn(x,\n",
    "              layer=None,\n",
    "              cv1_out=None,\n",
    "              cv1_filter=(1, 1),\n",
    "              cv1_strides=(1, 1),\n",
    "              cv2_out=None,\n",
    "              cv2_filter=(3, 3),\n",
    "              cv2_strides=(1, 1),\n",
    "              padding=None):\n",
    "    num = '' if cv2_out == None else '1'\n",
    "    tensor = Conv2D(cv1_out, cv1_filter, strides=cv1_strides, data_format='channels_first', name=layer+'_conv'+num)(x)\n",
    "    tensor = BatchNormalization(axis=1, epsilon=0.00001, name=layer+'_bn'+num)(tensor)\n",
    "    tensor = Activation('relu')(tensor)\n",
    "    if padding == None:\n",
    "        return tensor\n",
    "    tensor = ZeroPadding2D(padding=padding, data_format='channels_first')(tensor)\n",
    "    if cv2_out == None:\n",
    "        return tensor\n",
    "    tensor = Conv2D(cv2_out, cv2_filter, strides=cv2_strides, data_format='channels_first', name=layer+'_conv'+'2')(tensor)\n",
    "    tensor = BatchNormalization(axis=1, epsilon=0.00001, name=layer+'_bn'+'2')(tensor)\n",
    "    tensor = Activation('relu')(tensor)\n",
    "    return tensor\n",
    "\n",
    "WEIGHTS_mod = [\n",
    "  'conv1', 'bn1', 'conv2', 'bn2', 'conv3', 'bn3'\n",
    "]\n",
    "conv_shape = {\n",
    "  'conv1': [64, 3, 7, 7],\n",
    "  'conv2': [64, 64, 1, 1],\n",
    "  'conv3': [192, 64, 3, 3]\n",
    "}\n",
    "\n",
    "def load_weights_from_FaceNet(FRmodel):\n",
    "    # Load weights from csv files (which was exported from Openface torch model)\n",
    "    weights = WEIGHTS_mod\n",
    "    weights_dict = load_weights()\n",
    "\n",
    "    # Set layer weights of the model\n",
    "    for name in weights:\n",
    "        if FRmodel.get_layer(name) != None:\n",
    "            FRmodel.get_layer(name).set_weights(weights_dict[name])\n",
    "        elif model.get_layer(name) != None:\n",
    "            model.get_layer(name).set_weights(weights_dict[name])\n",
    "\n",
    "def load_weights():\n",
    "    # Set weights path\n",
    "    dirPath = './weights'\n",
    "    fileNames = filter(lambda f: not f.startswith('.'), os.listdir(dirPath))\n",
    "    paths = {}\n",
    "    weights_dict = {}\n",
    "\n",
    "    for n in fileNames:\n",
    "        paths[n.replace('.csv', '')] = dirPath + '/' + n\n",
    "\n",
    "    for name in WEIGHTS_mod:\n",
    "        if 'conv' in name:\n",
    "            conv_w = genfromtxt(paths[name + '_w'], delimiter=',', dtype=None)\n",
    "            conv_w = np.reshape(conv_w, conv_shape[name])\n",
    "            conv_w = np.transpose(conv_w, (2, 3, 1, 0))\n",
    "            conv_b = genfromtxt(paths[name + '_b'], delimiter=',', dtype=None)\n",
    "            weights_dict[name] = [conv_w, conv_b]     \n",
    "        elif 'bn' in name:\n",
    "            bn_w = genfromtxt(paths[name + '_w'], delimiter=',', dtype=None)\n",
    "            bn_b = genfromtxt(paths[name + '_b'], delimiter=',', dtype=None)\n",
    "            bn_m = genfromtxt(paths[name + '_m'], delimiter=',', dtype=None)\n",
    "            bn_v = genfromtxt(paths[name + '_v'], delimiter=',', dtype=None)\n",
    "            weights_dict[name] = [bn_w, bn_b, bn_m, bn_v]\n",
    "        elif 'dense' in name:\n",
    "            dense_w = genfromtxt(dirPath+'/dense_w.csv', delimiter=',', dtype=None)\n",
    "            dense_w = np.reshape(dense_w, (128, 736))\n",
    "            dense_w = np.transpose(dense_w, (1, 0))\n",
    "            dense_b = genfromtxt(dirPath+'/dense_b.csv', delimiter=',', dtype=None)\n",
    "            weights_dict[name] = [dense_w, dense_b]\n",
    "\n",
    "    return weights_dict\n",
    "\n",
    "def img_to_encoding(image_path, model):\n",
    "    img1 = cv2.imread(image_path, 1)\n",
    "    img = img1[...,::-1]\n",
    "    img = np.around(np.transpose(img, (2,0,1))/255.0, decimals=12)\n",
    "    x_train = np.array([img])\n",
    "    embedding = model.predict_on_batch(x_train)\n",
    "    print(\"Embedding shape is \", embedding.shape)\n",
    "    flat_input=embedding.flatten()\n",
    "    print(\"Flatten embedding \",flat_input.shape)\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 128)\n",
      "Inside triplet loss\n",
      "Tensor(\"loss_4/lambda_1_loss/strided_slice:0\", shape=(128,), dtype=float32)\n",
      "Tensor(\"loss_4/lambda_1_loss/strided_slice_1:0\", shape=(128,), dtype=float32)\n",
      "Tensor(\"loss_4/lambda_1_loss/strided_slice_2:0\", shape=(128,), dtype=float32)\n",
      "Entering the function\n"
     ]
    }
   ],
   "source": [
    "\n",
    "FRmodel = faceRecoModel(input_shape=(3, 350, 350))\n",
    "FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])\n",
    "load_weights_from_FaceNet(FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array([['Image1.png','Image1_modified.png','Image2.png'],\n",
    "   ['Image1.png','Image1_modified1.png','Image3.png'],\n",
    "   ['Image1.png','Image1_modified2.png','Image4.png'],\n",
    "   ['Image1.png','Image1_modified3.png','Image5.png']]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_array(image_path):\n",
    "    img1 = cv2.imread(image_path, 1)\n",
    "    img = img1[...,::-1]\n",
    "    img = np.around(np.transpose(img, (2,0,1))/255.0, decimals=12)\n",
    "    iarray = np.array([img]) \n",
    "    return iarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   #[Not used]\n",
    "    \n",
    "    positive_item_input = Input((1, ), name='positive_item_input')\n",
    "    negative_item_input = Input((1, ), name='negative_item_input')\n",
    "    user_input = Input((1, ), name='user_input')\n",
    "    # Shared embedding layer for positive and negative items\n",
    "    # item_embedding_layer = Embedding(\n",
    "    # num_items, latent_dim, name='item_embedding', input_length=1)\n",
    "    user_input = Input((1, ), name='user_input')\n",
    "    # positive_item_embedding = Flatten()(positive_item_input)\n",
    "    #negative_item_embedding = Flatten()(negative_item_input)\n",
    "    user_embedding = Flatten()(user_input)\n",
    "    print (user_embedding)\n",
    "    loss = np.zeros((user_input.shape))\n",
    "    \n",
    "    model = Model (input=[ user_input,positive_item_input,negative_item_input],\n",
    "                   output=loss)\n",
    "    model.compile(loss=triplet_loss, optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   #[Not used]\n",
    "X=np.array([['Image1.png','Image1_modified.png','Image2.png'],\n",
    "   ['Image1.png','Image1_modified1.png','Image3.png'],\n",
    "   ['Image1.png','Image1_modified2.png','Image4.png'],\n",
    "   ['Image1.png','Image1_modified3.png','Image5.png']]\n",
    "    )\n",
    "X_inputs=[]\n",
    "for uid,pid,nid in X:\n",
    "     X_triplet = {\n",
    "        'user_input':image_to_array(\"images/\"+uid),\n",
    "        'positive_item_input':image_to_array(\"images/\"+pid),\n",
    "        'negative_item_input':image_to_array(\"images/\"+nid)\n",
    "     }\n",
    "     X_inputs=X_inputs.append(X_triplet)\n",
    "    \n",
    "FRmodel.fit(inputs=X_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_images(image_path_original,image_path_check):\n",
    "    \n",
    "    imageA=mpimg.imread(image_path_original)\n",
    "    imageB=mpimg.imread(image_path_check)\n",
    "    fig = plt.figure(\"Comparison\")\n",
    " \n",
    "    # show first image\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(imageA, cmap = plt.cm.gray)\n",
    "    plt.axis(\"off\")\n",
    " \n",
    "    # show the second image\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    plt.imshow(imageB, cmap = plt.cm.gray)\n",
    "    plt.axis(\"off\")\n",
    " \n",
    "    # show the images\n",
    "    plt.show()\n",
    "\n",
    "def verify(image_path_original, image_path_check, model): \n",
    "    \n",
    "    encoding_original = img_to_encoding(image_path_original,model)\n",
    "    encoding_check = img_to_encoding(image_path_check,model)\n",
    "    \n",
    "    dist = np.linalg.norm(encoding_original-encoding_check)\n",
    "    dist = dist/256\n",
    "    # setup the figure\n",
    "    \n",
    "    show_images(image_path_original,image_path_check)\n",
    "    \n",
    "    print (\"Distance is \" , dist)\n",
    "    if dist < 1:\n",
    "        print(\"It's similar\")\n",
    "                \n",
    "    else:\n",
    "        print(\"It's not similar\")\n",
    "    return dist\n",
    "\n",
    "def verify_ml_approach(image_path_original,image_path_check):\n",
    "\n",
    "    dist = verify(image_path_original, image_path_check, FRmodel)\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape is  (1, 128)\n",
      "Flatten embedding  (128,)\n",
      "Embedding shape is  (1, 128)\n",
      "Flatten embedding  (128,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance is  0.002332814270630479\n",
      "It's similar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.002332814270630479"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify_ml_approach(\"images/image1.png\", \"images/image3.png\")\n",
    "#verify_ml_approach(\"images/Original.png\", \"images/Negative.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "verify_ml_approach(\"images/image1.png\", \"images/image1_modified.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_ml_approach(\"images/image1.png\", \"images/image1_modified1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_ml_approach(\"images/image1.png\", \"images/image1_modified2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_ml_approach(\"images/image1.png\", \"images/image1_modified3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_ml_approach(\"images/image1.png\",\"images/image9.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_ml_approach(\"images/image1.png\",\"images/image6.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = {}\n",
    "database[\"Image1\"] = img_to_encoding(\"images/Image1.png\", FRmodel)\n",
    "database[\"Image2\"] = img_to_encoding(\"images/Image2.png\", FRmodel)\n",
    "database[\"Image3\"] = img_to_encoding(\"images/Image3.png\", FRmodel)\n",
    "database[\"Image4\"] = img_to_encoding(\"images/Image4.png\", FRmodel)\n",
    "database[\"Image5\"] = img_to_encoding(\"images/Image5.png\", FRmodel)\n",
    "database[\"Image6\"] = img_to_encoding(\"images/Image6.png\", FRmodel)\n",
    "database[\"Image7\"] = img_to_encoding(\"images/Image7.png\", FRmodel)\n",
    "database[\"Image8\"] = img_to_encoding(\"images/Image8.png\", FRmodel)\n",
    "database[\"Image9\"] = img_to_encoding(\"images/Image9.png\", FRmodel)\n",
    "database[\"Image10\"] = img_to_encoding(\"images/Image10.png\", FRmodel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-69286b530831>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m               \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# starts training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(128,))\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "x = Dense(64, activation='relu')(inputs)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# This creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(data, labels)  # starts training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
