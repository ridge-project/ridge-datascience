{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from numpy import genfromtxt\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss as defined by formula (3)\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor images, of shape (None, 128)\n",
    "            positive -- the encodings for the positive images, of shape (None, 128)\n",
    "            negative -- the encodings for the negative images, of shape (None, 128)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    print(\"Inside triplet loss\")\n",
    "    \n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    print(anchor)\n",
    "    print(positive)\n",
    "    print(negative)\n",
    "    ### START CODE HERE ### (Ëœ 4 lines)\n",
    "    # Step 1: Compute the (encoding) distance between the anchor and the positive, you will need to sum over axis=-1\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)),axis=-1)\n",
    "    # Step 2: Compute the (encoding) distance between the anchor and the negative, you will need to sum over axis=-1\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)),axis=-1)\n",
    "    # Step 3: subtract the two previous distances and add alpha.\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist,neg_dist),alpha)\n",
    "    # Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.\n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss, 0.0))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    print(\"Entering the function\")\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faceRecoModel(input_shape):\n",
    "    \"\"\"\n",
    "    Implementation of the Inception model used for FaceNet\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "        \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    # First Block\n",
    "    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1')(X)\n",
    "    X = BatchNormalization(axis = 1, name = 'bn1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Zero-Padding + MAXPOOL\n",
    "    X = ZeroPadding2D((1, 1))(X)\n",
    "    X = MaxPooling2D((3, 3), strides = 2)(X)\n",
    "    \n",
    "    # Second Block\n",
    "    X = Conv2D(64, (1, 1), strides = (1, 1), name = 'conv2')(X)\n",
    "    X = BatchNormalization(axis = 1, epsilon=0.00001, name = 'bn2')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Zero-Padding + MAXPOOL\n",
    "    X = ZeroPadding2D((1, 1))(X)\n",
    "\n",
    "    # Second Block\n",
    "    X = Conv2D(192, (3, 3), strides = (1, 1), name = 'conv3')(X)\n",
    "    X = BatchNormalization(axis = 1, epsilon=0.00001, name = 'bn3')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Zero-Padding + MAXPOOL\n",
    "    X = ZeroPadding2D((1, 1))(X)\n",
    "    \n",
    "    X = MaxPooling2D(pool_size = 3, strides = 2)(X)\n",
    "    \n",
    "    X = AveragePooling2D(pool_size=(3, 3), strides=(1, 1), data_format='channels_first')(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(128, name='dense_layer')(X)\n",
    "    \n",
    "    # L2 normalization\n",
    "    X = Lambda(lambda  x: K.l2_normalize(x,axis=1))(X)\n",
    "    \n",
    "             \n",
    "    # Create model instance\n",
    "    model = Model(inputs = X_input, outputs = X, name='FaceRecoModel')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_FLOATX = 'float32'\n",
    "def variable(value, dtype=_FLOATX, name=None):\n",
    "    v = tf.Variable(np.asarray(value, dtype=dtype), name=name)\n",
    "    _get_session().run(v.initializer)\n",
    "    return v\n",
    "\n",
    "def shape(x):\n",
    "    return x.get_shape()\n",
    "\n",
    "def square(x):\n",
    "    return tf.square(x)\n",
    "\n",
    "def zeros(shape, dtype=_FLOATX, name=None):\n",
    "    return variable(np.zeros(shape), dtype, name)\n",
    "\n",
    "def concatenate(tensors, axis=-1):\n",
    "    if axis < 0:\n",
    "        axis = axis % len(tensors[0].get_shape())\n",
    "    return tf.concat(axis, tensors)\n",
    "\n",
    "def LRN2D(x):\n",
    "    return tf.nn.lrn(x, alpha=1e-4, beta=0.75)\n",
    "\n",
    "def conv2d_bn(x,\n",
    "              layer=None,\n",
    "              cv1_out=None,\n",
    "              cv1_filter=(1, 1),\n",
    "              cv1_strides=(1, 1),\n",
    "              cv2_out=None,\n",
    "              cv2_filter=(3, 3),\n",
    "              cv2_strides=(1, 1),\n",
    "              padding=None):\n",
    "    num = '' if cv2_out == None else '1'\n",
    "    tensor = Conv2D(cv1_out, cv1_filter, strides=cv1_strides, data_format='channels_first', name=layer+'_conv'+num)(x)\n",
    "    tensor = BatchNormalization(axis=1, epsilon=0.00001, name=layer+'_bn'+num)(tensor)\n",
    "    tensor = Activation('relu')(tensor)\n",
    "    if padding == None:\n",
    "        return tensor\n",
    "    tensor = ZeroPadding2D(padding=padding, data_format='channels_first')(tensor)\n",
    "    if cv2_out == None:\n",
    "        return tensor\n",
    "    tensor = Conv2D(cv2_out, cv2_filter, strides=cv2_strides, data_format='channels_first', name=layer+'_conv'+'2')(tensor)\n",
    "    tensor = BatchNormalization(axis=1, epsilon=0.00001, name=layer+'_bn'+'2')(tensor)\n",
    "    tensor = Activation('relu')(tensor)\n",
    "    return tensor\n",
    "\n",
    "WEIGHTS_mod = [\n",
    "  'conv1', 'bn1', 'conv2', 'bn2', 'conv3', 'bn3'\n",
    "]\n",
    "conv_shape = {\n",
    "  'conv1': [64, 3, 7, 7],\n",
    "  'conv2': [64, 64, 1, 1],\n",
    "  'conv3': [192, 64, 3, 3]\n",
    "}\n",
    "\n",
    "def load_weights_from_FaceNet(FRmodel):\n",
    "    # Load weights from csv files (which was exported from Openface torch model)\n",
    "    weights = WEIGHTS_mod\n",
    "    weights_dict = load_weights()\n",
    "\n",
    "    # Set layer weights of the model\n",
    "    for name in weights:\n",
    "        if FRmodel.get_layer(name) != None:\n",
    "            FRmodel.get_layer(name).set_weights(weights_dict[name])\n",
    "        elif model.get_layer(name) != None:\n",
    "            model.get_layer(name).set_weights(weights_dict[name])\n",
    "\n",
    "def load_weights():\n",
    "    # Set weights path\n",
    "    dirPath = './weights'\n",
    "    fileNames = filter(lambda f: not f.startswith('.'), os.listdir(dirPath))\n",
    "    paths = {}\n",
    "    weights_dict = {}\n",
    "\n",
    "    for n in fileNames:\n",
    "        paths[n.replace('.csv', '')] = dirPath + '/' + n\n",
    "\n",
    "    for name in WEIGHTS_mod:\n",
    "        if 'conv' in name:\n",
    "            conv_w = genfromtxt(paths[name + '_w'], delimiter=',', dtype=None)\n",
    "            conv_w = np.reshape(conv_w, conv_shape[name])\n",
    "            conv_w = np.transpose(conv_w, (2, 3, 1, 0))\n",
    "            conv_b = genfromtxt(paths[name + '_b'], delimiter=',', dtype=None)\n",
    "            weights_dict[name] = [conv_w, conv_b]     \n",
    "        elif 'bn' in name:\n",
    "            bn_w = genfromtxt(paths[name + '_w'], delimiter=',', dtype=None)\n",
    "            bn_b = genfromtxt(paths[name + '_b'], delimiter=',', dtype=None)\n",
    "            bn_m = genfromtxt(paths[name + '_m'], delimiter=',', dtype=None)\n",
    "            bn_v = genfromtxt(paths[name + '_v'], delimiter=',', dtype=None)\n",
    "            weights_dict[name] = [bn_w, bn_b, bn_m, bn_v]\n",
    "        elif 'dense' in name:\n",
    "            dense_w = genfromtxt(dirPath+'/dense_w.csv', delimiter=',', dtype=None)\n",
    "            dense_w = np.reshape(dense_w, (128, 736))\n",
    "            dense_w = np.transpose(dense_w, (1, 0))\n",
    "            dense_b = genfromtxt(dirPath+'/dense_b.csv', delimiter=',', dtype=None)\n",
    "            weights_dict[name] = [dense_w, dense_b]\n",
    "\n",
    "    return weights_dict\n",
    "\n",
    "def img_to_encoding(image_path, model):\n",
    "    img1 = cv2.imread(image_path, 1)\n",
    "    img = img1[...,::-1]\n",
    "    img = np.around(np.transpose(img, (2,0,1))/255.0, decimals=12)\n",
    "    x_train = np.array([img])\n",
    "    embedding = model.predict_on_batch(x_train)\n",
    "    flat_input=embedding.flatten()\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside triplet loss\n",
      "Tensor(\"loss_3/lambda_5_loss/strided_slice:0\", shape=(128,), dtype=float32)\n",
      "Tensor(\"loss_3/lambda_5_loss/strided_slice_1:0\", shape=(128,), dtype=float32)\n",
      "Tensor(\"loss_3/lambda_5_loss/strided_slice_2:0\", shape=(128,), dtype=float32)\n",
      "Entering the function\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_6 to have 4 dimensions, but got array with shape (32, 256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mcell_name\u001b[0m in \u001b[0;36masync-def-wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_6 to have 4 dimensions, but got array with shape (32, 256)"
     ]
    }
   ],
   "source": [
    "#def train_1st_model():\n",
    "    X,Y,m=load_train_data()\n",
    "    data,labels=prepare_train_data(X,Y,m)\n",
    "\n",
    "    FRmodel = faceRecoModel(input_shape=(3, 350, 350))\n",
    "    FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])\n",
    "    FRmodel.fit(data,labels,epochs=500 )  # starts training\n",
    "    score = FRmodel.evaluate(data,labels)\n",
    " #   return Smodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#FRmodel = faceRecoModel(input_shape=(3, 350, 350))\n",
    "#FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])\n",
    "#load_weights_from_FaceNet(FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    " def show_images(image_path_original,image_path_check):\n",
    "    \n",
    "    imageA=mpimg.imread(image_path_original)\n",
    "    imageB=mpimg.imread(image_path_check)\n",
    "    fig = plt.figure(\"Comparison\")\n",
    " \n",
    "    # show first image\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(imageA, cmap = plt.cm.gray)\n",
    "    plt.axis(\"off\")\n",
    " \n",
    "    # show the second image\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    plt.imshow(imageB, cmap = plt.cm.gray)\n",
    "    plt.axis(\"off\")\n",
    " \n",
    "    # show the images\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(image_path_original, image_path_check, model): \n",
    "    \n",
    "    encoding_original = img_to_encoding(image_path_original,model)\n",
    "    encoding_check = img_to_encoding(image_path_check,model)\n",
    "    \n",
    "    dist = np.linalg.norm(encoding_original-encoding_check)\n",
    "    dist = dist/256\n",
    "    # setup the figure\n",
    "    \n",
    "    show_images(image_path_original,image_path_check)\n",
    "    print (\"Distance is \" , dist)\n",
    "    if dist < 1:\n",
    "        print(\"It's similar\")          \n",
    "    else:\n",
    "        print(\"It's not similar\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def verify_ml_approach(image_path_original,image_path_check):\n",
    "    verify(image_path_original, image_path_check, FRmodel)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_train_data():\n",
    "    dataset = np.genfromtxt(\"data.csv\", dtype=\"U\" ,delimiter=\",\" )\n",
    "    X = dataset[:,0:2]\n",
    "    Y = dataset[:,2]\n",
    "    Y= [int(i) for i in Y]\n",
    "    m=len(Y)\n",
    "    return X,Y,m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getimagePath(name):\n",
    "    return('images/'+str(name)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimpleModel():\n",
    "    inputs=Input(shape=(256,))\n",
    "    # a layer instance is callable on a tensor, and returns a tensor\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    predictions = Dense(11, activation='softmax')(x)\n",
    "\n",
    "    # This creates a model that includes\n",
    "    # the Input layer and three Dense layers\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSeqModel():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(11, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_data(X,Y,m,classes=11):\n",
    "    dat=np.zeros((1,256))\n",
    "    data=np.zeros((m,256))\n",
    "    labels=np.zeros((m,classes))\n",
    "    row=0\n",
    "    for [ori,chk] in X:\n",
    "        ori_encoding=img_to_encoding(getimagePath(ori), FRmodel)\n",
    "        chk_encoding=img_to_encoding(getimagePath(chk), FRmodel)\n",
    "        dat=np.concatenate((ori_encoding,chk_encoding),axis=1)\n",
    "        data[row,:]=dat\n",
    "        labels[row,Y[row]]=1\n",
    "        row=row+1  \n",
    "    return data,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3 -1 -2 -3]\n"
     ]
    }
   ],
   "source": [
    "a=[1,2,3]\n",
    "b=-1*np.array(a)\n",
    "c=np.concatenate((a,b))\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_2nd_model():\n",
    "    X,Y,m=load_train_data()\n",
    "    data,labels=prepare_train_data(X,Y,m)\n",
    "\n",
    "    Smodel=getSimpleModel()\n",
    "    Smodel.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    Smodel.fit(data,labels,epochs=500 )  # starts training\n",
    "    score = Smodel.evaluate(data,labels)\n",
    "    return Smodel\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 2.3700 - acc: 0.2188\n",
      "Epoch 2/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 2.2218 - acc: 0.5625\n",
      "Epoch 3/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 2.0884 - acc: 0.5938\n",
      "Epoch 4/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 1.9622 - acc: 0.6250\n",
      "Epoch 5/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 1.8432 - acc: 0.5938\n",
      "Epoch 6/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 1.7401 - acc: 0.6250\n",
      "Epoch 7/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 1.6553 - acc: 0.6250\n",
      "Epoch 8/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 1.5835 - acc: 0.6562\n",
      "Epoch 9/500\n",
      "32/32 [==============================] - 0s 218us/step - loss: 1.5334 - acc: 0.6562\n",
      "Epoch 10/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 1.4942 - acc: 0.6250\n",
      "Epoch 11/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 1.4626 - acc: 0.6875\n",
      "Epoch 12/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 1.4380 - acc: 0.6250\n",
      "Epoch 13/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 1.4160 - acc: 0.7188\n",
      "Epoch 14/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 1.3978 - acc: 0.6250\n",
      "Epoch 15/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 1.3791 - acc: 0.7188\n",
      "Epoch 16/500\n",
      "32/32 [==============================] - 0s 187us/step - loss: 1.3655 - acc: 0.6250\n",
      "Epoch 17/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 1.3516 - acc: 0.7188\n",
      "Epoch 18/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 1.3386 - acc: 0.6250\n",
      "Epoch 19/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 1.3246 - acc: 0.7188\n",
      "Epoch 20/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 1.3111 - acc: 0.6250\n",
      "Epoch 21/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 1.2980 - acc: 0.7188\n",
      "Epoch 22/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 1.2841 - acc: 0.6562\n",
      "Epoch 23/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 1.2735 - acc: 0.7188\n",
      "Epoch 24/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 1.2610 - acc: 0.6562\n",
      "Epoch 25/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 1.2525 - acc: 0.7188\n",
      "Epoch 26/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 1.2393 - acc: 0.6875\n",
      "Epoch 27/500\n",
      "32/32 [==============================] - 0s 187us/step - loss: 1.2311 - acc: 0.7188\n",
      "Epoch 28/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 1.2174 - acc: 0.6875\n",
      "Epoch 29/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 1.2083 - acc: 0.7188\n",
      "Epoch 30/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 1.1969 - acc: 0.7188\n",
      "Epoch 31/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 1.1882 - acc: 0.7188\n",
      "Epoch 32/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 1.1763 - acc: 0.7188\n",
      "Epoch 33/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 1.1673 - acc: 0.7188\n",
      "Epoch 34/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 1.1566 - acc: 0.7188\n",
      "Epoch 35/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 1.1485 - acc: 0.7188\n",
      "Epoch 36/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 1.1390 - acc: 0.7188\n",
      "Epoch 37/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 1.1307 - acc: 0.7188\n",
      "Epoch 38/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 1.1229 - acc: 0.7188\n",
      "Epoch 39/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 1.1145 - acc: 0.7188\n",
      "Epoch 40/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 1.1057 - acc: 0.7188\n",
      "Epoch 41/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 1.0967 - acc: 0.7188\n",
      "Epoch 42/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 1.0868 - acc: 0.7188\n",
      "Epoch 43/500\n",
      "32/32 [==============================] - 0s 95us/step - loss: 1.0778 - acc: 0.7188\n",
      "Epoch 44/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 1.0676 - acc: 0.7188\n",
      "Epoch 45/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 1.0594 - acc: 0.7188\n",
      "Epoch 46/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 1.0517 - acc: 0.7188\n",
      "Epoch 47/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 1.0445 - acc: 0.7188\n",
      "Epoch 48/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 1.0359 - acc: 0.7188\n",
      "Epoch 49/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 1.0286 - acc: 0.7188\n",
      "Epoch 50/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 1.0213 - acc: 0.7188\n",
      "Epoch 51/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 1.0128 - acc: 0.7188\n",
      "Epoch 52/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 1.0051 - acc: 0.7188\n",
      "Epoch 53/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.9955 - acc: 0.7188\n",
      "Epoch 54/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.9871 - acc: 0.7188\n",
      "Epoch 55/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.9769 - acc: 0.7188\n",
      "Epoch 56/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.9691 - acc: 0.7188\n",
      "Epoch 57/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.9622 - acc: 0.7188\n",
      "Epoch 58/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.9558 - acc: 0.7188\n",
      "Epoch 59/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.9474 - acc: 0.7188\n",
      "Epoch 60/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.9406 - acc: 0.7188\n",
      "Epoch 61/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.9316 - acc: 0.7188\n",
      "Epoch 62/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.9248 - acc: 0.7188\n",
      "Epoch 63/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.9155 - acc: 0.7188\n",
      "Epoch 64/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.9088 - acc: 0.7188\n",
      "Epoch 65/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.8988 - acc: 0.7188\n",
      "Epoch 66/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.8915 - acc: 0.7188\n",
      "Epoch 67/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.8833 - acc: 0.7188\n",
      "Epoch 68/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.8766 - acc: 0.7188\n",
      "Epoch 69/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.8682 - acc: 0.7188\n",
      "Epoch 70/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.8640 - acc: 0.7188\n",
      "Epoch 71/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.8535 - acc: 0.7188\n",
      "Epoch 72/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.8470 - acc: 0.7188\n",
      "Epoch 73/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.8362 - acc: 0.7188\n",
      "Epoch 74/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.8293 - acc: 0.7188\n",
      "Epoch 75/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.8198 - acc: 0.7188\n",
      "Epoch 76/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.8118 - acc: 0.7188\n",
      "Epoch 77/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.8041 - acc: 0.7188\n",
      "Epoch 78/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.7965 - acc: 0.7188\n",
      "Epoch 79/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.7881 - acc: 0.7188\n",
      "Epoch 80/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.7813 - acc: 0.7188\n",
      "Epoch 81/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.7722 - acc: 0.7188\n",
      "Epoch 82/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.7682 - acc: 0.7188\n",
      "Epoch 83/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.7565 - acc: 0.7188\n",
      "Epoch 84/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.7505 - acc: 0.7188\n",
      "Epoch 85/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.7392 - acc: 0.7188\n",
      "Epoch 86/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.7331 - acc: 0.7188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.7218 - acc: 0.7188\n",
      "Epoch 88/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.7149 - acc: 0.7188\n",
      "Epoch 89/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.7045 - acc: 0.7188\n",
      "Epoch 90/500\n",
      "32/32 [==============================] - 0s 218us/step - loss: 0.6968 - acc: 0.7500\n",
      "Epoch 91/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.6880 - acc: 0.7188\n",
      "Epoch 92/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.6829 - acc: 0.7500\n",
      "Epoch 93/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.6748 - acc: 0.7188\n",
      "Epoch 94/500\n",
      "32/32 [==============================] - 0s 187us/step - loss: 0.6721 - acc: 0.7500\n",
      "Epoch 95/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.6591 - acc: 0.7188\n",
      "Epoch 96/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.6529 - acc: 0.7500\n",
      "Epoch 97/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.6405 - acc: 0.7500\n",
      "Epoch 98/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.6320 - acc: 0.7500\n",
      "Epoch 99/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.6234 - acc: 0.7500\n",
      "Epoch 100/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.6176 - acc: 0.8125\n",
      "Epoch 101/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.6102 - acc: 0.8125\n",
      "Epoch 102/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.6086 - acc: 0.7500\n",
      "Epoch 103/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.6041 - acc: 0.8125\n",
      "Epoch 104/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.6098 - acc: 0.7500\n",
      "Epoch 105/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.5895 - acc: 0.8438\n",
      "Epoch 106/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.5791 - acc: 0.7500\n",
      "Epoch 107/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.5668 - acc: 0.8125\n",
      "Epoch 108/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.5583 - acc: 0.8125\n",
      "Epoch 109/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.5504 - acc: 0.8125\n",
      "Epoch 110/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.5454 - acc: 0.8125\n",
      "Epoch 111/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.5385 - acc: 0.8438\n",
      "Epoch 112/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.5349 - acc: 0.8125\n",
      "Epoch 113/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.5292 - acc: 0.8750\n",
      "Epoch 114/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.5268 - acc: 0.8125\n",
      "Epoch 115/500\n",
      "32/32 [==============================] - 0s 249us/step - loss: 0.5165 - acc: 0.8750\n",
      "Epoch 116/500\n",
      "32/32 [==============================] - 0s 186us/step - loss: 0.5131 - acc: 0.8125\n",
      "Epoch 117/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.5013 - acc: 0.8750\n",
      "Epoch 118/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.4958 - acc: 0.8125\n",
      "Epoch 119/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.4860 - acc: 0.9062\n",
      "Epoch 120/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.4814 - acc: 0.8125\n",
      "Epoch 121/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.4746 - acc: 0.8750\n",
      "Epoch 122/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.4716 - acc: 0.8125\n",
      "Epoch 123/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.4645 - acc: 0.9062\n",
      "Epoch 124/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.4591 - acc: 0.8125\n",
      "Epoch 125/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.4494 - acc: 0.9375\n",
      "Epoch 126/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.4422 - acc: 0.8438\n",
      "Epoch 127/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.4345 - acc: 0.9375\n",
      "Epoch 128/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.4277 - acc: 0.9062\n",
      "Epoch 129/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.4226 - acc: 0.9375\n",
      "Epoch 130/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.4229 - acc: 0.8750\n",
      "Epoch 131/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.4203 - acc: 0.9375\n",
      "Epoch 132/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.4249 - acc: 0.9062\n",
      "Epoch 133/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.4039 - acc: 0.9375\n",
      "Epoch 134/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.3961 - acc: 0.9375\n",
      "Epoch 135/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.3883 - acc: 0.9375\n",
      "Epoch 136/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.3840 - acc: 0.9375\n",
      "Epoch 137/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.3803 - acc: 0.9375\n",
      "Epoch 138/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.3769 - acc: 0.9062\n",
      "Epoch 139/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.3703 - acc: 0.9375\n",
      "Epoch 140/500\n",
      "32/32 [==============================] - 0s 124us/step - loss: 0.3649 - acc: 0.9375\n",
      "Epoch 141/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.3566 - acc: 0.9375\n",
      "Epoch 142/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.3517 - acc: 0.9375\n",
      "Epoch 143/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.3460 - acc: 0.9375\n",
      "Epoch 144/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.3453 - acc: 0.9375\n",
      "Epoch 145/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.3413 - acc: 0.9688\n",
      "Epoch 146/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.3427 - acc: 0.9375\n",
      "Epoch 147/500\n",
      "32/32 [==============================] - 0s 187us/step - loss: 0.3300 - acc: 0.9688\n",
      "Epoch 148/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.3246 - acc: 0.9375\n",
      "Epoch 149/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.3174 - acc: 0.9688\n",
      "Epoch 150/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.3144 - acc: 0.9375\n",
      "Epoch 151/500\n",
      "32/32 [==============================] - 0s 250us/step - loss: 0.3085 - acc: 0.9688\n",
      "Epoch 152/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.3044 - acc: 0.9375\n",
      "Epoch 153/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.2983 - acc: 0.9688\n",
      "Epoch 154/500\n",
      "32/32 [==============================] - 0s 249us/step - loss: 0.2941 - acc: 0.9375\n",
      "Epoch 155/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.2887 - acc: 0.9688\n",
      "Epoch 156/500\n",
      "32/32 [==============================] - 0s 249us/step - loss: 0.2864 - acc: 0.9375\n",
      "Epoch 157/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.2809 - acc: 0.9688\n",
      "Epoch 158/500\n",
      "32/32 [==============================] - 0s 218us/step - loss: 0.2805 - acc: 0.9375\n",
      "Epoch 159/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.2732 - acc: 0.9688\n",
      "Epoch 160/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.2706 - acc: 0.9375\n",
      "Epoch 161/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.2635 - acc: 0.9688\n",
      "Epoch 162/500\n",
      "32/32 [==============================] - 0s 187us/step - loss: 0.2597 - acc: 0.9688\n",
      "Epoch 163/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.2554 - acc: 0.9688\n",
      "Epoch 164/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.2539 - acc: 0.9688\n",
      "Epoch 165/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.2486 - acc: 0.9688\n",
      "Epoch 166/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.2446 - acc: 1.0000\n",
      "Epoch 167/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.2377 - acc: 0.9688\n",
      "Epoch 168/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.2332 - acc: 0.9688\n",
      "Epoch 169/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.2286 - acc: 0.9688\n",
      "Epoch 170/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.2268 - acc: 0.9688\n",
      "Epoch 171/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.2226 - acc: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.2226 - acc: 0.9688\n",
      "Epoch 173/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.2178 - acc: 1.0000\n",
      "Epoch 174/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.2158 - acc: 0.9688\n",
      "Epoch 175/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.2088 - acc: 0.9688\n",
      "Epoch 176/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.2059 - acc: 1.0000\n",
      "Epoch 177/500\n",
      "32/32 [==============================] - 0s 187us/step - loss: 0.2010 - acc: 0.9688\n",
      "Epoch 178/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.1983 - acc: 0.9688\n",
      "Epoch 179/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.1954 - acc: 0.9688\n",
      "Epoch 180/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.1929 - acc: 0.9688\n",
      "Epoch 181/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.1880 - acc: 0.9688\n",
      "Epoch 182/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.1840 - acc: 0.9688\n",
      "Epoch 183/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.1805 - acc: 0.9688\n",
      "Epoch 184/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.1769 - acc: 0.9688\n",
      "Epoch 185/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.1747 - acc: 0.9688\n",
      "Epoch 186/500\n",
      "32/32 [==============================] - 0s 187us/step - loss: 0.1718 - acc: 0.9688\n",
      "Epoch 187/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.1708 - acc: 0.9688\n",
      "Epoch 188/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.1675 - acc: 0.9688\n",
      "Epoch 189/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.1656 - acc: 0.9688\n",
      "Epoch 190/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.1606 - acc: 0.9688\n",
      "Epoch 191/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.1576 - acc: 0.9688\n",
      "Epoch 192/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.1539 - acc: 0.9688\n",
      "Epoch 193/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.1510 - acc: 1.0000\n",
      "Epoch 194/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.1482 - acc: 1.0000\n",
      "Epoch 195/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.1482 - acc: 0.9688\n",
      "Epoch 196/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.1473 - acc: 1.0000\n",
      "Epoch 197/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.1475 - acc: 0.9688\n",
      "Epoch 198/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.1419 - acc: 0.9688\n",
      "Epoch 199/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.1391 - acc: 0.9688\n",
      "Epoch 200/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.1346 - acc: 0.9688\n",
      "Epoch 201/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.1309 - acc: 0.9688\n",
      "Epoch 202/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.1282 - acc: 1.0000\n",
      "Epoch 203/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.1258 - acc: 0.9688\n",
      "Epoch 204/500\n",
      "32/32 [==============================] - 0s 187us/step - loss: 0.1237 - acc: 1.0000\n",
      "Epoch 205/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.1221 - acc: 0.9688\n",
      "Epoch 206/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.1206 - acc: 0.9688\n",
      "Epoch 207/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.1206 - acc: 0.9688\n",
      "Epoch 208/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.1192 - acc: 0.9688\n",
      "Epoch 209/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.1188 - acc: 0.9688\n",
      "Epoch 210/500\n",
      "32/32 [==============================] - 0s 63us/step - loss: 0.1163 - acc: 0.9688\n",
      "Epoch 211/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.1126 - acc: 0.9688\n",
      "Epoch 212/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.1091 - acc: 1.0000\n",
      "Epoch 213/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.1066 - acc: 0.9688\n",
      "Epoch 214/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.1041 - acc: 1.0000\n",
      "Epoch 215/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.1021 - acc: 1.0000\n",
      "Epoch 216/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.1003 - acc: 1.0000\n",
      "Epoch 217/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0989 - acc: 1.0000\n",
      "Epoch 218/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0984 - acc: 1.0000\n",
      "Epoch 219/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0967 - acc: 1.0000\n",
      "Epoch 220/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0960 - acc: 0.9688\n",
      "Epoch 221/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0948 - acc: 1.0000\n",
      "Epoch 222/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0944 - acc: 0.9688\n",
      "Epoch 223/500\n",
      "32/32 [==============================] - 0s 63us/step - loss: 0.0908 - acc: 0.9688\n",
      "Epoch 224/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.0884 - acc: 0.9688\n",
      "Epoch 225/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0866 - acc: 1.0000\n",
      "Epoch 226/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0856 - acc: 0.9688\n",
      "Epoch 227/500\n",
      "32/32 [==============================] - 0s 249us/step - loss: 0.0843 - acc: 0.9688\n",
      "Epoch 228/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0840 - acc: 0.9688\n",
      "Epoch 229/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0833 - acc: 0.9688\n",
      "Epoch 230/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0824 - acc: 0.9688\n",
      "Epoch 231/500\n",
      "32/32 [==============================] - 0s 124us/step - loss: 0.0806 - acc: 0.9688\n",
      "Epoch 232/500\n",
      "32/32 [==============================] - 0s 187us/step - loss: 0.0790 - acc: 0.9688\n",
      "Epoch 233/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0771 - acc: 0.9688\n",
      "Epoch 234/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0752 - acc: 0.9688\n",
      "Epoch 235/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0739 - acc: 1.0000\n",
      "Epoch 236/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0728 - acc: 0.9688\n",
      "Epoch 237/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0717 - acc: 1.0000\n",
      "Epoch 238/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0713 - acc: 0.9688\n",
      "Epoch 239/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0701 - acc: 1.0000\n",
      "Epoch 240/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.0692 - acc: 0.9688\n",
      "Epoch 241/500\n",
      "32/32 [==============================] - 0s 218us/step - loss: 0.0683 - acc: 0.9688\n",
      "Epoch 242/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0675 - acc: 0.9688\n",
      "Epoch 243/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0667 - acc: 0.9688\n",
      "Epoch 244/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0657 - acc: 0.9688\n",
      "Epoch 245/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0646 - acc: 0.9688\n",
      "Epoch 246/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0634 - acc: 0.9688\n",
      "Epoch 247/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0624 - acc: 1.0000\n",
      "Epoch 248/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0614 - acc: 0.9688\n",
      "Epoch 249/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0602 - acc: 1.0000\n",
      "Epoch 250/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0592 - acc: 0.9688\n",
      "Epoch 251/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0584 - acc: 1.0000\n",
      "Epoch 252/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0576 - acc: 0.9688\n",
      "Epoch 253/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0569 - acc: 1.0000\n",
      "Epoch 254/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0573 - acc: 0.9688\n",
      "Epoch 255/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0577 - acc: 0.9688\n",
      "Epoch 256/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0585 - acc: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0562 - acc: 0.9688\n",
      "Epoch 258/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0543 - acc: 0.9688\n",
      "Epoch 259/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0524 - acc: 1.0000\n",
      "Epoch 260/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0510 - acc: 1.0000\n",
      "Epoch 261/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0501 - acc: 1.0000\n",
      "Epoch 262/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0495 - acc: 1.0000\n",
      "Epoch 263/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0490 - acc: 1.0000\n",
      "Epoch 264/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0483 - acc: 1.0000\n",
      "Epoch 265/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0481 - acc: 1.0000\n",
      "Epoch 266/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0486 - acc: 0.9688\n",
      "Epoch 267/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.0493 - acc: 0.9688\n",
      "Epoch 268/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0507 - acc: 0.9688\n",
      "Epoch 269/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0485 - acc: 0.9688\n",
      "Epoch 270/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0462 - acc: 0.9688\n",
      "Epoch 271/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0448 - acc: 1.0000\n",
      "Epoch 272/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0438 - acc: 1.0000\n",
      "Epoch 273/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0430 - acc: 1.0000\n",
      "Epoch 274/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0423 - acc: 1.0000\n",
      "Epoch 275/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0420 - acc: 1.0000\n",
      "Epoch 276/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0420 - acc: 1.0000\n",
      "Epoch 277/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0419 - acc: 1.0000\n",
      "Epoch 278/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0421 - acc: 0.9688\n",
      "Epoch 279/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0423 - acc: 1.0000\n",
      "Epoch 280/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0427 - acc: 0.9688\n",
      "Epoch 281/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0420 - acc: 0.9688\n",
      "Epoch 282/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.0418 - acc: 0.9688\n",
      "Epoch 283/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0396 - acc: 1.0000\n",
      "Epoch 284/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0382 - acc: 1.0000\n",
      "Epoch 285/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0374 - acc: 1.0000\n",
      "Epoch 286/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0366 - acc: 1.0000\n",
      "Epoch 287/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.0361 - acc: 1.0000\n",
      "Epoch 288/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.0358 - acc: 1.0000\n",
      "Epoch 289/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0355 - acc: 1.0000\n",
      "Epoch 290/500\n",
      "32/32 [==============================] - 0s 592us/step - loss: 0.0353 - acc: 1.0000\n",
      "Epoch 291/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0354 - acc: 1.0000\n",
      "Epoch 292/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0362 - acc: 1.0000\n",
      "Epoch 293/500\n",
      "32/32 [==============================] - 0s 281us/step - loss: 0.0368 - acc: 1.0000\n",
      "Epoch 294/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0370 - acc: 0.9688\n",
      "Epoch 295/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0355 - acc: 1.0000\n",
      "Epoch 296/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0340 - acc: 1.0000\n",
      "Epoch 297/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0328 - acc: 1.0000\n",
      "Epoch 298/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0320 - acc: 1.0000\n",
      "Epoch 299/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0315 - acc: 1.0000\n",
      "Epoch 300/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0311 - acc: 1.0000\n",
      "Epoch 301/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0308 - acc: 1.0000\n",
      "Epoch 302/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0306 - acc: 1.0000\n",
      "Epoch 303/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0306 - acc: 1.0000\n",
      "Epoch 304/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0311 - acc: 1.0000\n",
      "Epoch 305/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0314 - acc: 1.0000\n",
      "Epoch 306/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0320 - acc: 1.0000\n",
      "Epoch 307/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0308 - acc: 1.0000\n",
      "Epoch 308/500\n",
      "32/32 [==============================] - 0s 312us/step - loss: 0.0300 - acc: 1.0000\n",
      "Epoch 309/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0287 - acc: 1.0000\n",
      "Epoch 310/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0279 - acc: 1.0000\n",
      "Epoch 311/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0272 - acc: 1.0000\n",
      "Epoch 312/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0267 - acc: 1.0000\n",
      "Epoch 313/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0265 - acc: 1.0000\n",
      "Epoch 314/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0262 - acc: 1.0000\n",
      "Epoch 315/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0262 - acc: 1.0000\n",
      "Epoch 316/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0263 - acc: 1.0000\n",
      "Epoch 317/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0268 - acc: 1.0000\n",
      "Epoch 318/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0278 - acc: 1.0000\n",
      "Epoch 319/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0278 - acc: 1.0000\n",
      "Epoch 320/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0274 - acc: 1.0000\n",
      "Epoch 321/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0255 - acc: 1.0000\n",
      "Epoch 322/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0242 - acc: 1.0000\n",
      "Epoch 323/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0234 - acc: 1.0000\n",
      "Epoch 324/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0228 - acc: 1.0000\n",
      "Epoch 325/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0224 - acc: 1.0000\n",
      "Epoch 326/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0221 - acc: 1.0000\n",
      "Epoch 327/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0220 - acc: 1.0000\n",
      "Epoch 328/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0220 - acc: 1.0000\n",
      "Epoch 329/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0221 - acc: 1.0000\n",
      "Epoch 330/500\n",
      "32/32 [==============================] - 0s 187us/step - loss: 0.0232 - acc: 1.0000\n",
      "Epoch 331/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0238 - acc: 1.0000\n",
      "Epoch 332/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0246 - acc: 1.0000\n",
      "Epoch 333/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0231 - acc: 1.0000\n",
      "Epoch 334/500\n",
      "32/32 [==============================] - 0s 219us/step - loss: 0.0214 - acc: 1.0000\n",
      "Epoch 335/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0204 - acc: 1.0000\n",
      "Epoch 336/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0197 - acc: 1.0000\n",
      "Epoch 337/500\n",
      "32/32 [==============================] - 0s 250us/step - loss: 0.0191 - acc: 1.0000\n",
      "Epoch 338/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0188 - acc: 1.0000\n",
      "Epoch 339/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0185 - acc: 1.0000\n",
      "Epoch 340/500\n",
      "32/32 [==============================] - 0s 280us/step - loss: 0.0184 - acc: 1.0000\n",
      "Epoch 341/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0184 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0188 - acc: 1.0000\n",
      "Epoch 343/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0193 - acc: 1.0000\n",
      "Epoch 344/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.0211 - acc: 1.0000\n",
      "Epoch 345/500\n",
      "32/32 [==============================] - 0s 187us/step - loss: 0.0207 - acc: 1.0000\n",
      "Epoch 346/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0201 - acc: 1.0000\n",
      "Epoch 347/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0183 - acc: 1.0000\n",
      "Epoch 348/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0170 - acc: 1.0000\n",
      "Epoch 349/500\n",
      "32/32 [==============================] - 0s 63us/step - loss: 0.0162 - acc: 1.0000\n",
      "Epoch 350/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0158 - acc: 1.0000\n",
      "Epoch 351/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0154 - acc: 1.0000\n",
      "Epoch 352/500\n",
      "32/32 [==============================] - 0s 187us/step - loss: 0.0152 - acc: 1.0000\n",
      "Epoch 353/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0150 - acc: 1.0000\n",
      "Epoch 354/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0150 - acc: 1.0000\n",
      "Epoch 355/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0151 - acc: 1.0000\n",
      "Epoch 356/500\n",
      "32/32 [==============================] - 0s 218us/step - loss: 0.0160 - acc: 1.0000\n",
      "Epoch 357/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0168 - acc: 1.0000\n",
      "Epoch 358/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0183 - acc: 1.0000\n",
      "Epoch 359/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0175 - acc: 1.0000\n",
      "Epoch 360/500\n",
      "32/32 [==============================] - 0s 187us/step - loss: 0.0164 - acc: 1.0000\n",
      "Epoch 361/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0146 - acc: 1.0000\n",
      "Epoch 362/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0136 - acc: 1.0000\n",
      "Epoch 363/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0130 - acc: 1.0000\n",
      "Epoch 364/500\n",
      "32/32 [==============================] - 0s 124us/step - loss: 0.0126 - acc: 1.0000\n",
      "Epoch 365/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0123 - acc: 1.0000\n",
      "Epoch 366/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.0121 - acc: 1.0000\n",
      "Epoch 367/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0119 - acc: 1.0000\n",
      "Epoch 368/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0118 - acc: 1.0000\n",
      "Epoch 369/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0119 - acc: 1.0000\n",
      "Epoch 370/500\n",
      "32/32 [==============================] - 0s 187us/step - loss: 0.0123 - acc: 1.0000\n",
      "Epoch 371/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0131 - acc: 1.0000\n",
      "Epoch 372/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0151 - acc: 1.0000\n",
      "Epoch 373/500\n",
      "32/32 [==============================] - 0s 124us/step - loss: 0.0156 - acc: 1.0000\n",
      "Epoch 374/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0147 - acc: 1.0000\n",
      "Epoch 375/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0126 - acc: 1.0000\n",
      "Epoch 376/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0113 - acc: 1.0000\n",
      "Epoch 377/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0104 - acc: 1.0000\n",
      "Epoch 378/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0100 - acc: 1.0000\n",
      "Epoch 379/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0097 - acc: 1.0000\n",
      "Epoch 380/500\n",
      "32/32 [==============================] - 0s 187us/step - loss: 0.0094 - acc: 1.0000\n",
      "Epoch 381/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0093 - acc: 1.0000\n",
      "Epoch 382/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0091 - acc: 1.0000\n",
      "Epoch 383/500\n",
      "32/32 [==============================] - 0s 187us/step - loss: 0.0090 - acc: 1.0000\n",
      "Epoch 384/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.0090 - acc: 1.0000\n",
      "Epoch 385/500\n",
      "32/32 [==============================] - 0s 187us/step - loss: 0.0091 - acc: 1.0000\n",
      "Epoch 386/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0096 - acc: 1.0000\n",
      "Epoch 387/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0108 - acc: 1.0000\n",
      "Epoch 388/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0129 - acc: 1.0000\n",
      "Epoch 389/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0134 - acc: 1.0000\n",
      "Epoch 390/500\n",
      "32/32 [==============================] - 0s 187us/step - loss: 0.0120 - acc: 1.0000\n",
      "Epoch 391/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0096 - acc: 1.0000\n",
      "Epoch 392/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0083 - acc: 1.0000\n",
      "Epoch 393/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0077 - acc: 1.0000\n",
      "Epoch 394/500\n",
      "32/32 [==============================] - 0s 219us/step - loss: 0.0073 - acc: 1.0000\n",
      "Epoch 395/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.0071 - acc: 1.0000\n",
      "Epoch 396/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0070 - acc: 1.0000\n",
      "Epoch 397/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0068 - acc: 1.0000\n",
      "Epoch 398/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0067 - acc: 1.0000\n",
      "Epoch 399/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.0066 - acc: 1.0000\n",
      "Epoch 400/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0064 - acc: 1.0000\n",
      "Epoch 401/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0063 - acc: 1.0000\n",
      "Epoch 402/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.0063 - acc: 1.0000\n",
      "Epoch 403/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0064 - acc: 1.0000\n",
      "Epoch 404/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0071 - acc: 1.0000\n",
      "Epoch 405/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.0087 - acc: 1.0000\n",
      "Epoch 406/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0115 - acc: 1.0000\n",
      "Epoch 407/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0124 - acc: 1.0000\n",
      "Epoch 408/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0096 - acc: 1.0000\n",
      "Epoch 409/500\n",
      "32/32 [==============================] - 0s 187us/step - loss: 0.0069 - acc: 1.0000\n",
      "Epoch 410/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0057 - acc: 1.0000\n",
      "Epoch 411/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0052 - acc: 1.0000\n",
      "Epoch 412/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.0051 - acc: 1.0000\n",
      "Epoch 413/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0049 - acc: 1.0000\n",
      "Epoch 414/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0049 - acc: 1.0000\n",
      "Epoch 415/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0048 - acc: 1.0000\n",
      "Epoch 416/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0047 - acc: 1.0000\n",
      "Epoch 417/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0046 - acc: 1.0000\n",
      "Epoch 418/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.0045 - acc: 1.0000\n",
      "Epoch 419/500\n",
      "32/32 [==============================] - 0s 187us/step - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 420/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 421/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 422/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 423/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 424/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 425/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0039 - acc: 1.0000\n",
      "Epoch 426/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.0042 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427/500\n",
      "32/32 [==============================] - 0s 218us/step - loss: 0.0054 - acc: 1.0000\n",
      "Epoch 428/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0090 - acc: 1.0000\n",
      "Epoch 429/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0133 - acc: 1.0000\n",
      "Epoch 430/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0093 - acc: 1.0000\n",
      "Epoch 431/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0047 - acc: 1.0000\n",
      "Epoch 432/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 433/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 434/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 435/500\n",
      "32/32 [==============================] - 0s 155us/step - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 436/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 437/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 438/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 439/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 440/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 441/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 442/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 443/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 444/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 445/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 446/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 447/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 448/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 449/500\n",
      "32/32 [==============================] - 0s 187us/step - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 450/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 451/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 452/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 453/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0065 - acc: 1.0000\n",
      "Epoch 454/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.0082 - acc: 1.0000\n",
      "Epoch 455/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0057 - acc: 1.0000\n",
      "Epoch 456/500\n",
      "32/32 [==============================] - 0s 187us/step - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 457/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 458/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 459/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 460/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 461/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 462/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 463/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 464/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 465/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 466/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 467/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 468/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 469/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 470/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 471/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 472/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 473/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 474/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 475/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 476/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 477/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 478/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 479/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0046 - acc: 1.0000\n",
      "Epoch 480/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0049 - acc: 1.0000\n",
      "Epoch 481/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 482/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 483/500\n",
      "32/32 [==============================] - 0s 187us/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 484/500\n",
      "32/32 [==============================] - 0s 187us/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 485/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 486/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 487/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 488/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 489/500\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 490/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 491/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 9.8494e-04 - acc: 1.0000\n",
      "Epoch 492/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 9.6629e-04 - acc: 1.0000\n",
      "Epoch 493/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 9.4888e-04 - acc: 1.0000\n",
      "Epoch 494/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 9.3109e-04 - acc: 1.0000\n",
      "Epoch 495/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 9.1382e-04 - acc: 1.0000\n",
      "Epoch 496/500\n",
      "32/32 [==============================] - 0s 93us/step - loss: 8.9892e-04 - acc: 1.0000\n",
      "Epoch 497/500\n",
      "32/32 [==============================] - 0s 125us/step - loss: 8.7786e-04 - acc: 1.0000\n",
      "Epoch 498/500\n",
      "32/32 [==============================] - 0s 187us/step - loss: 8.6124e-04 - acc: 1.0000\n",
      "Epoch 499/500\n",
      "32/32 [==============================] - 0s 94us/step - loss: 8.6382e-04 - acc: 1.0000\n",
      "Epoch 500/500\n",
      "32/32 [==============================] - 0s 62us/step - loss: 8.5663e-04 - acc: 1.0000\n",
      "32/32 [==============================] - 0s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "Smodel=train_2nd_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    " def predict(image_path_original,image_path_check,FRmodel,Smodel):\n",
    "    encoding_original = img_to_encoding(image_path_original,FRmodel)\n",
    "    encoding_check = img_to_encoding(image_path_check,FRmodel)\n",
    "    dat=np.concatenate((encoding_original,encoding_check),axis=1)\n",
    "    \n",
    "    prediction = Smodel.predict_on_batch(dat)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_2_models(image_path_original,image_path_check):\n",
    "    show_images(image_path_original,image_path_check)\n",
    "    print(\"Scoring...............................\")\n",
    "    prediction = predict(image_path_original, image_path_check, FRmodel,Smodel)\n",
    "    score = np.argmax(prediction)\n",
    "    print(\"Score of similarity is \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance is  0.0019403264159336686\n",
      "It's similar\n"
     ]
    }
   ],
   "source": [
    "verify_ml_approach(\"images/image2.png\", \"images/image1_modified.png\")\n",
    "#predict_2_models(\"images/image2.png\", \"images/image1_modified.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance is  0.0013758967397734523\n",
      "It's similar\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring...............................\n",
      "Score of similarity is  8\n"
     ]
    }
   ],
   "source": [
    "verify_ml_approach(\"images/image1.png\", \"images/image1_modified1.png\")\n",
    "predict_2_models(\"images/image1.png\", \"images/image1_modified1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance is  0.0\n",
      "It's similar\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring...............................\n",
      "Score of similarity is  10\n"
     ]
    }
   ],
   "source": [
    "verify_ml_approach(\"images/image3.png\", \"images/image3.png\")\n",
    "predict_2_models(\"images/image3.png\", \"images/image3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance is  0.0019299464765936136\n",
      "It's similar\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring...............................\n",
      "Score of similarity is  10\n"
     ]
    }
   ],
   "source": [
    "verify_ml_approach(\"images/image5.png\", \"images/image6.png\")\n",
    "predict_2_models(\"images/image5.png\", \"images/image6.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
